{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 574, 700])\n",
      "(574, 700, 3)\n",
      "(2, 574, 700)\n",
      "(574, 700, 3)\n",
      "(574, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad # todo: actually explore these other types of CAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "target_layers = [model.layer4[-1]]\n",
    "path = '../../data/GradCAM/cat.jpg'\n",
    "image = np.array(Image.open(path))\n",
    "image_float_np = np.float32(image) / 255\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "image_tensor = torchvision.transforms.ToTensor()(image).unsqueeze(0)\n",
    "input_tensor = torch.cat([image_tensor, image_tensor], dim=0)\n",
    "print(input_tensor.shape)\n",
    "print(image.shape)\n",
    "# input_tensor = input_tensor.unsqueeze(0)\n",
    "with GradCAM(model=model, target_layers=target_layers, use_cuda=False) as cam:\n",
    "    category = 281  # presumably, this is the class cat in imagenet?\n",
    "    targets = [ClassifierOutputTarget(category), ClassifierOutputTarget(category)]\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    print(grayscale_cam.shape)\n",
    "for i in range(grayscale_cam.shape[0]):\n",
    "    print(image.shape)\n",
    "    visualization = show_cam_on_image(image/255, grayscale_cam[i, :], use_rgb=True)\n",
    "    Image.fromarray(visualization).show()\n",
    "# visualization = show_cam_on_image(image/255, grayscale_cam, use_rgb=True)\n",
    "# Image.fromarray(visualization)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "Epoch 0/14\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m finetune\u001B[38;5;241m.\u001B[39mFEATURE_EXTRACT \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m finetune\u001B[38;5;241m.\u001B[39minitialize_model()\n\u001B[0;32m---> 12\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mfinetune\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinetune_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Studie/Master/2021-2022/REDI/Project/REDI-utwente/src/awesome_GNN/finetune.py:281\u001B[0m, in \u001B[0;36mfinetune_model\u001B[0;34m(model)\u001B[0m\n\u001B[1;32m    279\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mSGD(params_to_update, lr\u001B[38;5;241m=\u001B[39mLR, momentum\u001B[38;5;241m=\u001B[39mMOMENTUM)\n\u001B[1;32m    280\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m--> 281\u001B[0m model, hist \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_dataloaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNUM_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_inception\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_inception\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model, hist\n",
      "File \u001B[0;32m~/Documents/Studie/Master/2021-2022/REDI/Project/REDI-utwente/src/awesome_GNN/finetune.py:230\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001B[0m\n\u001B[1;32m    227\u001B[0m             optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# statistics\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    231\u001B[0m     running_corrects \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(preds \u001B[38;5;241m==\u001B[39m labels\u001B[38;5;241m.\u001B[39mdata)\n\u001B[1;32m    233\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloaders[phase]\u001B[38;5;241m.\u001B[39mget_dataset)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gradcam\n",
    "import finetune\n",
    "import importlib\n",
    "importlib.reload(gradcam)\n",
    "importlib.reload(finetune)\n",
    "\n",
    "finetune.NUM_CLASSES = 196\n",
    "finetune.CLASSIFIER_NAME = 'resnet'\n",
    "finetune.NUM_EPOCHS = 15\n",
    "finetune.FEATURE_EXTRACT = False\n",
    "model = finetune.initialize_model()\n",
    "model = finetune.finetune_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gradcam.train_distractor(None,model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}